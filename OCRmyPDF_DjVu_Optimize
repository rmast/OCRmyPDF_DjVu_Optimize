#!/usr/bin/python3
# OCRmyPDF Optimizatopm plugin by Robert Mast inspired by James R. Barlow: github.com/jbarlow83, Merlijn Wajer <merlijn@archive.org>
# enabling OCRmyPDF to optimize PDF's as good as Open Sourced parts of DjVu can do.
# As some DjVu-software-patents have expired there might even be some additional room for improvement for anyone who is able to understand them deeply.

"""Built-in plugin to implement PDF page optimization."""
import statistics
import itertools
import logging
import os
import sys
from sauvola import binarise_sauvola
import matplotlib.pyplot as plt

import mpl_toolkits.mplot3d as m3d
from skimage.restoration import estimate_sigma
import warnings
from sklearn import linear_model
import numpy as np
import matplotlib
matplotlib.use("TkAgg")
import argparse
from pathlib import Path
from typing import (
    Callable,
    Dict,
    Iterator,
    List,
    MutableSet,
    NamedTuple,
    NewType,
    Optional,
    Sequence,
    Tuple,
)

import pandas as pd

from ocrmypdf import PdfContext, hookimpl
from ocrmypdf._concurrent import Executor, SerialExecutor
from ocrmypdf._exec import jbig2enc, pngquant
from ocrmypdf._pipeline import get_pdf_save_settings
from ocrmypdf.cli import numeric
from ocrmypdf.optimize import optimize
from ocrmypdf.subprocess import check_external_program
from subprocess import check_call, check_output
import os.path
import tempfile
import shutil
import threading
from collections import defaultdict, Counter
from os import fspath
from zlib import compress

import img2pdf
from pikepdf import (
    Dictionary,
    Name,
    Object,
    ObjectStreamMode,
    Pdf,
    PdfError,
    PdfImage,
    Stream,
    UnsupportedImageTypeError,
)
from PIL import Image

#from ocrmypdf._jobcontext import PdfContext
from ocrmypdf.exceptions import OutputFileAccessError
from ocrmypdf.helpers import IMG2PDF_KWARGS, safe_symlink

import io

import fitz

from hocr.parse import hocr_page_iterator, hocr_page_to_word_data
from internetarchivepdf.const import DENOISE_FAST, JPEG2000_IMPL_KAKADU, \
    JPEG2000_IMPL_PILLOW, COMPRESSOR_JPEG, COMPRESSOR_JPEG2000
#from internetarchivepdf.mrc import create_mrc_hocr_components

sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/didjvu/lib")


#sys.path.insert(0, './didjvu/lib')

from didjvu.lib import djvu_support as djvu
#from . import filetype
#from . import fs
from didjvu.lib import gamera_support as gamera
from didjvu.lib import ipc
#from . import templates
from didjvu.lib import temporary
from didjvu.lib import utils
#from . import xmp
from didjvu.lib import didjvu

from internetarchivepdf.mrc import encode_mrc_images
# TODO:
# - Deal with arbitrary rotation and matrix sizes when placing the image back
# - Decide if we want to ignore transparent images alltogether
# - Give black/white images (1bit images) special treatment
# - Stabilise and clean up the code, the many clean_contents
# - 

def remove_images(doc, page, unwanted):
    un_list = [b"/%s Do" % u.encode() for u in unwanted]
    #page.clean_contents()  # unify / format the commands
    xref=page.get_contents()[0]  # get its XREF
    cont=page.read_contents().splitlines()  # read commands as list of lines
    for i in range(len(cont)):  # walk thru the lines
        if cont[i] in un_list:  # invokes an unwanted image
            cont[i] = b""  # remove command
    doc.update_stream(xref, b"\n".join(cont))  # replace cleaned command object
    #page.clean_contents()  # removes now unreferenced images from page definition


def compress_page_images(doc, page, hocr_word_data=[]):
    page.clean_contents()
    imgs = page.get_images(full=True)

    to_remove_xrefs = []
    to_insert = []

    for img_data in imgs:
        xref = img_data[0]
        #print(img_data)
        orig_img = doc.extract_image(xref)
        to_remove_xrefs.append(xref)
        bbox = page.get_image_bbox(img_data)
        #print(bbox)

        imgfd = io.BytesIO()
        imgfd.write(orig_img["image"])
        pil_image = Image.open(imgfd)
        pil_image.load()
        # TODO: if greyscale or 1bit, treat differently
        # TODO: force 1bit mode?
        #print('image mode', pil_image.mode)
        #print('image size', pil_image.size)

        imgfd.close()

        dpi = orig_img['xres']

        djvu.require_cli()
        gamera.init()
        mrc_gen = create_mrc_hocr_components(pil_image, hocr_word_data,
        #mrc_gen = create_mrc_hocr_components(pil_image, [],
            denoise_mask=DENOISE_FAST,
            bg_downsample=3
            )

        fg_slope = 44500
        bg_slope = 44250
        # with pillow
        #mask_f, bg_f, bg_s, fg_f, fg_s = encode_mrc_images(mrc_gen,
        #        jpeg2000_implementation=JPEG2000_IMPL_PILLOW,
        #        bg_compression_flags=['quality_mode:"rates";quality_layers:[250]'],
        #        #fg_compression_flags=['quality_mode:"rates";quality_layers:[300]'],
        #        fg_compression_flags=[''],
        #        )

        # with jpegoptim
        #mask_f, bg_f, bg_s, fg_f, fg_s = encode_mrc_images(mrc_gen,
        #        mrc_image_format=COMPRESSOR_JPEG,
        #        bg_compression_flags=['-S30'],
        #        fg_compression_flags=['-S20'],
        #        )

        mask_f, bg_f, bg_s, fg_f, fg_s = encode_mrc_images(mrc_gen,
                jpeg2000_implementation=JPEG2000_IMPL_KAKADU,
                bg_compression_flags=['-slope', str(bg_slope)],
                #fg_compression_flags=['-slope', str(fg_slope)],
                fg_compression_flags=['-com','hoi'],
                )

        # TODO: maybe we can replace the existing image with the background image
        # here
        bg_contents = open(bg_f, 'rb').read()
        fg_contents = open(fg_f, 'rb').read()
        mask_contents = open(mask_f, 'rb').read()

        os.remove(mask_f)
        os.remove(bg_f)
        os.remove(fg_f)

        to_insert.append([
            {'bbox': bbox, 'stream': bg_contents, 'mask': None, 'overlay': False},
            {'bbox': bbox, 'stream': fg_contents, 'mask': mask_contents, 'overlay': True}
        ])


    page.clean_contents()
    for xref in to_remove_xrefs:
        imgs = page.get_images(full=True)
        for img_data in imgs:
            if img_data[0] == xref:
                remove_images(doc, page, [img_data[7]])
    page.clean_contents()

    for insert in to_insert:
        img1 = insert[0]
        img2 = insert[1]
        page.insert_image(img1['bbox'], stream=img1['stream'],
                mask=img1['mask'], overlay=img1['overlay'], alpha=0)
        page.insert_image(img2['bbox'], stream=img2['stream'],
                mask=img2['mask'], overlay=img2['overlay'], alpha=0)
        #page.clean_contents()

    page.clean_contents()

log = logging.getLogger(__name__)
DEBUG = True

@hookimpl
def add_options(parser):
    pass

@hookimpl
def check_options(options):
    pass

@hookimpl
def optimize_pdf(
    input_pdf: Path,
    output_pdf: Path,
    context: PdfContext,
    executor: Executor,
    linearize: bool,
) -> Tuple[Path, Sequence[str]]:
    save_settings = dict(
        linearize=linearize,
        **get_pdf_save_settings(context.options.output_type),
    )
    result_path = optimizeR(input_pdf, output_pdf, context, save_settings, executor)
    messages = []
    if context.options.optimize == 0:
        messages.append("Optimization was disabled.")
    else:
        image_optimizers = {
            'jbig2': jbig2enc.available(),
            'pngquant': pngquant.available(),
        }
        for name, available in image_optimizers.items():
            if not available:
                messages.append(
                    f"The optional dependency '{name}' was not found, so some image "
                    f"optimizations could not be attempted."
                )
    return result_path, messages


@hookimpl
def is_optimization_enabled(context: PdfContext) -> bool:
    return True

# Â© 2018 James R. Barlow: github.com/jbarlow83
#
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.

"""Post-processing image optimization of OCR PDFs."""



log = logging.getLogger(__name__)



DEFAULT_EXECUTOR = SerialExecutor()


def optimizeR(
    input_file: Path,
    output_file: Path,
    context,
    save_settings,
    executor: Executor = DEFAULT_EXECUTOR,
) -> Path:
    options = context.options
    log.info(
        "In OptimizeR aanbeland"
    )

    tmpd = tempfile.mkdtemp()
    pdfmeta = os.path.join(tmpd, 'pdfmeta.json')
    pdfhocr = os.path.join(tmpd, 'pdfhocr.html')

    if DEBUG:
        stde = sys.stderr
    else:
        stde = open(os.devnull, 'wb')

    out = check_output(['pdf-metadata-json', input_file], stderr=stde)
    with open(pdfmeta, 'wb+') as fd:
        fd.write(out)

    out = check_output(['pdf-to-hocr', '-f', input_file, '-J', pdfmeta], stderr=stde)
    with open(pdfhocr, 'wb+') as fd:
        fd.write(out)
    
    hocr_iter = hocr_page_iterator(pdfhocr)

    doc = fitz.open(input_file)
    outfile = output_file
    
    for page in doc:
        hocr_page = next(hocr_iter)
        hocr_word_data = hocr_page_to_word_data(hocr_page)
    
        compress_page_images(doc, page, hocr_word_data=hocr_word_data)
    
        page.clean_contents()
    
    doc.save(outfile, deflate=True, pretty=True, garbage=2)

    oldsize = os.path.getsize(input_file)
    newsize = os.path.getsize(output_file)
    compression_ratio  = oldsize / newsize
    print('Compression factor:', compression_ratio, file=sys.stderr)

    input_size = input_file.stat().st_size
    output_size = output_file.stat().st_size
    if output_size == 0:
        raise OutputFileAccessError(
            f"Output file not created after optimizing. We probably ran "
            f"out of disk space in the temporary folder: {tempfile.gettempdir()}."
        )
    savings = 1 - output_size / input_size

    if savings < 0:
        log.info(
            "Image optimization did not improve the file - "
            "optimizations will not be used"
        )
        return input_file
    else:
        return output_file

class OptionsStruct(NamedTuple):
    subsample: int
    slices: None
    crcb: int


subsample3 = OptionsStruct(3,[100],djvu.CRCB.full)
subsample12 = OptionsStruct(12,[100],djvu.CRCB.full)

# TODO: Reduce amount of memory active at one given point (keep less images in
# memory, write to disk sooner, etc), careful with numpy <-> PIL conversions
def create_mrc_hocr_components(image, hocr_word_data,
                               dpi=None,
                               downsample=None,
                               bg_downsample=None,
                               fg_downsample=None,
                               denoise_mask=None, timing_data=None,
                               errors=None):
    """
    Create the MRC components: mask, foreground and background

    Args:

    * image (PIL.Image): Image to be decomposed
    * hocr_word_data: OCR data about found text on the page
    * downsample (int): factor by which the OCR data is to be downsampled
    * bg_downsample (int): if the background image should be downscaled
    * denoise_mask (bool): Whether to denoise the image if it is deemed too
      noisy
    * timing_data: Optional timing data to log individual timing data to.
    * errors: Optional argument (of type set) with encountered runtime errors

    Returns a tuple of the components, as numpy arrays: (mask, foreground,
    background)
    """
    #npimage = np.array(image.convert('RGB'),dtype=np.byte)
    grayimg = image
    if image.mode != 'L':
        #t = time()
        grayimg = image.convert('L')
        #if timing_data is not None:
        #    timing_data.append(('grey_conversion', time() - t))

    width_, height_ = image.size

    mask_arr = np.array(Image.new('1', image.size))

    # Modifies mask_arr in place
    create_hocr_mask(grayimg, image, mask_arr, hocr_word_data, downsample=downsample,
                     dpi=dpi, timing_data=timing_data)
    grayimgf = np.array(grayimg, dtype=np.float32)

    #width_, height_ = image.size

    gamera_image = gamera._from_pil(image)
    mask2 = gamera.methods['djvu'](gamera_image)
    mask3 = mask2.to_greyscale()
    #mask3 = _image_conversion.to_greyscale(mask2)
    mask = gamera.to_pil_1bpp(mask3)
    mask_arr = np.array(mask)

    mask_inv = np.invert(mask_arr)

    yield mask_inv

    fg_djvu = didjvu.make_layer(gamera_image, mask3, didjvu.subsample_fg, subsample12)
    fg_ppm  = djvu_to_ppm(fg_djvu)
    foreground_arr = np.array(Image.open(fg_ppm))

    yield foreground_arr
    foreground_arr = None

    bg_djvu = didjvu.make_layer(gamera_image, mask3, didjvu.subsample_bg, subsample3)
    bg_ppm  = djvu_to_ppm(bg_djvu)
    background_arr = np.array(Image.open(bg_ppm))

    yield background_arr
    return

def create_hocr_mask(img, predataimage, mask_arr, hocr_word_data, downsample=None, dpi=None, timing_data=None):
    image_width, image_height = img.size
    np_img = np.array(img)

    #t = time()

    for paragraph in hocr_word_data:
        for line in paragraph['lines']:
            coords = line['bbox']

            line_text = ' '.join([word['text'] for word in line['words']])
            line_confs = [word['confidence'] for word in line['words']]
            line_conf = sum(line_confs) / len(line_confs)

            if line_text.strip() == '' or line_conf < 20:
                continue

            if downsample is not None:
                coords = [int(x/downsample) for x in coords]
            else:
                coords = [int(x) for x in coords]

            left, top, right, bottom = coords
            # This can happen if we downsample and round to int
            if left == right or top == bottom:
                continue

            if (left >= right) or (top >= bottom):
                print('Invalid bounding box: (%d, %d, %d, %d)' % (left, top, right, bottom), file=sys.stderr)
                continue

            if (left < 0) or (right > image_width) or (top < 0) or (bottom > image_height):
                print('Invalid bounding box outside image: (%d, %d, %d, %d)' % (left, top, right, bottom), file=sys.stderr)
                continue

            new_img = np.array(predataimage.crop((left, top, right, bottom)))

            new_imgt = new_img.transpose(2, 0, 1).reshape(3, -1)
            new_imgt2 = new_imgt.transpose()
            new_imgt2R = new_imgt2[:, [0,1]]
            new_imgt2B = new_imgt2[:, [1,2]]
            new_imgt2RB = new_imgt2[:, [0, 2]]

            c = Counter(map(tuple, new_imgt2))
            cR = Counter(map(tuple, new_imgt2R))
            cB = Counter(map(tuple, new_imgt2B))
            cRB = Counter(map(tuple, new_imgt2RB))
            res = np.c_[np.array(list(c.keys())), list(c.values())].T.T
            #np.savetxt("/home/rmast/res.txt", res)

            resR = np.c_[np.array(list(cR.keys())), list(cR.values())].T.T
            #np.savetxt("/home/rmast/resR.txt", resR)
            resB = np.c_[np.array(list(cB.keys())), list(cB.values())].T.T
            resRB = np.c_[np.array(list(cRB.keys())), list(cRB.values())].T.T
            r = res[:,[0]]
            rR = resR[:, [0]]
            #rRB = resRB[:, [0]]

            g = res[:,[1]]
            gR = resR[:,[1]]
            swR = resR[:,[2]].reshape(-1)
            gB = resB[:, [0]]
            bB = resB[:, [1]]
            swB = resB[:, [2]].reshape(-1)
            ols = linear_model.LinearRegression()
            modelR = ols.fit(rR, gR, swR)
            responseR = modelR.predict(rR)

            ############################################## Evaluate ############################################

            r2 = modelR.score(rR, gR, swR)

            modelB = ols.fit(bB, gB, swB)
            responseB = modelB.predict(bB)

            ############################################## Evaluate ############################################

            b2 = modelB.score(bB, gB, swB)


            plt.style.use('default')
            plt.style.use('ggplot')

            fig, ax = plt.subplots(figsize=(4, 4))
            #fig, ax = plt.subplots(gridspec_kw = {'height_ratios': swR})
            ax.plot(rR, responseR, color='k', label='Regression model')
            ax.scatter(rR, gR, swR, edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data')
            ax.set_ylabel(coords, fontsize=14)
            ax.set_xlabel(line_text, fontsize=14)
            ax.text(0.8, 0.1, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
                    transform=ax.transAxes, color='grey', alpha=0.5)
#            ax.legend(facecolor='white', fontsize=11)
            ax.set_title('$R^2= %.2f$' % r2, fontsize=18)

            fig.tight_layout()
            plt.draw()
            plt.show()
            plt.draw()

            np_lineimg = np_img[top:bottom,left:right]
            # Simple grayscale invert
            np_lineimg_invert = 255 - np.copy(np_lineimg)

            # XXX: If you tweak k, you must tweak the various ratio and sigma's
            # based on the test images
            k = 0.1
            thres = threshold_image(np_lineimg, dpi, k)
            ones = np.count_nonzero(thres)
            zero = (img.size[0] * img.size[1]) - ones
            ratio = (ones/(zero+ones))*100

            thres_invert = threshold_image(np_lineimg_invert, dpi, k)
            ones_i = np.count_nonzero(thres_invert)
            zero_i = (img.size[0] * img.size[1]) - ones
            inv_ratio = (ones_i/(zero_i+ones_i))*100

            if ratio < 0.3 or inv_ratio < 0.3:
                th = None

                if inv_ratio > 0.2 and ratio < 0.2:
                    th = thres
                else:
                    # mean_estimate_sigma is expensive, so let's only do it if
                    # we need to

                    ratio_sigma = mean_estimate_sigma(thres)
                    inv_ratio_sigma = mean_estimate_sigma(thres_invert)

                    # Prefer ratio over inv_ratio by a bit
                    if inv_ratio < 0.3 and inv_ratio < ratio and \
                    (inv_ratio_sigma < ratio_sigma or \
                    (ratio_sigma < 0.1 and inv_ratio_sigma < 0.1)):
                        th = thres_invert
                    elif ratio < 0.2:
                        th = thres
            else:
                perc_larger = 0.
                if inv_ratio != 0.0:
                    perc_larger = (ratio / inv_ratio) * 100
                if perc_larger < 50:
                    th = thres
                else:
                    th = thres_invert
                if th is not None:
                    mask_arr[top:bottom, left:right] = th

# skimage throws useless UserWarnings in various functions
def mean_estimate_sigma(arr):
    with warnings.catch_warnings():
        warnings.simplefilter('ignore')
        return np.mean(estimate_sigma(arr))


def threshold_image(img, dpi, k=0.34):
    """
    Perform Sauvola binarisation on the given image

    Args:

    * img (np.ndarray): input image array
    * dpi (int): dpi for Sauvola, used to calculate window size if not None
    * k (float): k parameter, defaults to 0.34

    Returns binarised numpy.ndarray
    """
    window_size = 51

    if dpi is not None:
        window_size = int(dpi / 4)
        if window_size % 2 == 0:
            window_size += 1

    h, w = img.shape
    out_img = np.ndarray(img.shape, dtype=np.bool)
    out_img = np.reshape(out_img, w * h)
    in_img = np.reshape(img, w * h)

    binarise_sauvola(in_img, out_img, w, h, window_size, window_size, k, 128)
    out_img = np.reshape(out_img, (h, w))
    # TODO: optimise this, we can do it in binarise_sauvola
    out_img = np.invert(out_img)

    return out_img

    #if timing_data is not None:
        #timing_data.append(('hocr_mask_gen', time() - t))

def djvu_to_ppm(djvu_file):
     # TODO: Use Multichunk.
     ppm_file = temporary.file(suffix='.ppm')
     args = ['ddjvu','-format=ppm', djvu_file.name, ppm_file.name]
     with open(os.devnull, 'wb') as dev_null:
         return utils.Proxy(ppm_file, ipc.Subprocess(args, stderr=dev_null).wait, [djvu_file])

def main(infile, outfile):
    from shutil import copy  # pylint: disable=import-outside-toplevel
    from tempfile import TemporaryDirectory  # pylint: disable=import-outside-toplevel

    infile = Path(infile)

    with TemporaryDirectory() as tmpdir:
        context = PdfContext(None, tmpdir, infile, None, None)
        tmpout = Path(tmpdir) / 'out.pdf'
        optimizeR(
            infile,
            tmpout,
            context,
            dict(
                compress_streams=True,
                preserve_pdfa=True,
                object_stream_mode=ObjectStreamMode.generate,
            ),
        )
        copy(fspath(tmpout), fspath(outfile))


if __name__ == '__main__':
    main(sys.argv[1], sys.argv[2])
